{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detction from documents using n-gram profiles\n",
    "\n",
    "This notebook is an attempt at building an n-gram profile based language detector inspired by [N-gram-based text categorization Cavnar, Trenkle (1994)](https://sdmines.sdsmt.edu/upload/directory/materials/12247_20070403135416.pdf).\n",
    "\n",
    "\n",
    "\n",
    "#### BibTex entry\n",
    "```bibtex\n",
    "@inproceedings{Cavnar1994NgrambasedTC,\n",
    "  title={N-gram-based text categorization},\n",
    "  author={William B. Cavnar and John M. Trenkle},\n",
    "  year={1994},\n",
    "  url={https://api.semanticscholar.org/CorpusID:170740}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core concept\n",
    "\n",
    "According to the Zipf's Law, the most dominant words in a language are lesser in frequency than their more frequent yet less dominant counterparts. N-gram profiles are built on the idea of the ranking of the most prominent n-grams in a language.\n",
    "\n",
    "Let's assume that we have a corpus $C$ of $N$ languages. For each language $L$ in the $C$, we can then create the ranking of the most common n-grams, which will act as the n-gram profile, $R_l$ for $l$. Once the profiles for all languages have been computed, we can infer on a held out corpus, containing $S$ sentences. For each sentence $s$ in the corpus, we first create the n-gram profile of $s$, $R_s$. Then, we measure the distance in the rankings of the n-grams in $R_s$ against the n-gram profiles of all the languages. In the end, the language which will have the least distance is selected as the predicted result. For our prediction target $y_l$, \n",
    "\n",
    "$$\n",
    "y_l = min(R_{s_i} - [R_{L_1} , R_{L_2}, ... , R_{N}])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using this small corpus from Kaggle titled [Language Detection](https://www.kaggle.com/code/basilb2s/language-detection-using-nlp). It contains 17 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'examples', 'isLiveDataset', 'rai'}\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [Metadata(Language Detection)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although humans are part of nature, human acti...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1] The word nature is borrowed from the Old F...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text language\n",
       "0   Nature, in the broadest sense, is the natural...  English\n",
       "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2  The study of nature is a large, if not the onl...  English\n",
       "3  Although humans are part of nature, human acti...  English\n",
       "4  [1] The word nature is borrowed from the Old F...  English"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_URL = \"https://www.kaggle.com/datasets/basilb2s/language-detection/croissant/download\"\n",
    "\n",
    "def get_croissant_dataset(dataset_url: str = DATASET_URL) -> pd.DataFrame:\n",
    "    # Fetch the Croissant JSON-LD\n",
    "    croissant_dataset = mlc.Dataset(dataset_url)\n",
    "\n",
    "    # Check what record sets are in the dataset\n",
    "    record_sets = croissant_dataset.metadata.record_sets\n",
    "\n",
    "    # Fetch the records and put them in a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        croissant_dataset.records(record_set=record_sets[0].uuid))\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns={\"Language+Detection.csv/Text\": \"text\",\n",
    "              \"Language+Detection.csv/Language\": \"language\"}, inplace=True)\n",
    "    \n",
    "    # convert the binary strings to utf-8\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "    df[\"language\"] = df[\"language\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = get_croissant_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language to index dictionary\n",
    "\n",
    "I'm assigning an integer id to each of the unique target languages in the dataset, which can then be used as index while creating language specific arrays later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English', 'Malayalam', 'Hindi', 'Tamil', 'Portugeese', 'French',\n",
       "       'Dutch', 'Spanish', 'Greek', 'Russian', 'Danish', 'Italian',\n",
       "       'Turkish', 'Sweedish', 'Arabic', 'German', 'Kannada'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_languages = df[\"language\"].unique()\n",
    "unique_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swedish and Portuguese are misspelled here. I am going to fix it before proceeding any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': 0,\n",
       " 'Malayalam': 1,\n",
       " 'Hindi': 2,\n",
       " 'Tamil': 3,\n",
       " 'Portuguese': 4,\n",
       " 'French': 5,\n",
       " 'Dutch': 6,\n",
       " 'Spanish': 7,\n",
       " 'Greek': 8,\n",
       " 'Russian': 9,\n",
       " 'Danish': 10,\n",
       " 'Italian': 11,\n",
       " 'Turkish': 12,\n",
       " 'Swedish': 13,\n",
       " 'Arabic': 14,\n",
       " 'German': 15,\n",
       " 'Kannada': 16}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_spelling(lang: str) -> str:\n",
    "    \"\"\"Fix the spelling of the language name\"\"\"\n",
    "    spelling = {\n",
    "        \"Portugeese\": \"Portuguese\",\n",
    "        \"Sweedish\": \"Swedish\",\n",
    "    }\n",
    "    return spelling.get(lang, lang)\n",
    "\n",
    "\n",
    "df[\"language\"] = df[\"language\"].apply(fix_spelling)\n",
    "unique_languages = df[\"language\"].unique()\n",
    "\n",
    "language_to_index = {language: index for index,\n",
    "                     language in enumerate(unique_languages)}\n",
    "language_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts, languages = df[\"text\"], df[\"language\"]\n",
    "train_texts, test_texts, train_languages, test_languages = train_test_split(texts, languages, test_size=0.2, random_state=42)\n",
    "\n",
    "assert len(train_texts) == len(train_languages)\n",
    "assert len(test_texts) == len(test_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Language:\n",
    "    id: int\n",
    "    name: str\n",
    "    profile: dict    \n",
    "    def __eq__(self, other: \"Language\") -> bool:\n",
    "        # two languages are equal if their profiles are the same\n",
    "        return np.array_equal(self.profile, other.profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would be a good time to check for which languages in the corpus, there's no stopwords list. This is important since the whole approach relies on finding the dominant n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stopwords for Malayalam\n",
      "No stopwords for Hindi\n",
      "No stopwords for Tamil\n",
      "No stopwords for Kannada\n"
     ]
    }
   ],
   "source": [
    "for language_name in language_to_index.keys():\n",
    "    try:\n",
    "        sw = stopwords.words(language_name)\n",
    "    except Exception:\n",
    "        print(f\"No stopwords for {language_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NGrams:\n",
    "    n: int\n",
    "    language: str\n",
    "    grams: list[str]\n",
    "    \n",
    "    def __eq__(self, other: \"NGrams\") -> bool:\n",
    "        # two n-grams are equal if their n and the language are the same\n",
    "        return self.n == other.n and self.language == other.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class NGramProfileClassifier:\n",
    "    def __init__(self, n: int, mapping: dict[str, int], most_common_n_grams: int = 25):\n",
    "        self.n = n\n",
    "        self.mapping = mapping\n",
    "        self.most_common_n_grams = most_common_n_grams\n",
    "        \n",
    "        # a dictionary of languages and their n-gram profiles\n",
    "        self.languages = {}\n",
    "        self.n_grams = {}\n",
    "        self.__populate()\n",
    "        \n",
    "        # number of languages\n",
    "        self.n_languages = len(self.mapping)\n",
    "        \n",
    "        \n",
    "    def __populate(self):\n",
    "        for language, index in self.mapping.items():\n",
    "            # lower case the language name\n",
    "            language = language.lower()\n",
    "            self.languages[language] = Language(index, language, {})\n",
    "            self.n_grams[language] = NGrams(n=self.n, language=language, grams=[])\n",
    "            \n",
    "            \n",
    "    def __process_single(self, text: str):\n",
    "        # tokenize the text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # remove stopwords\n",
    "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "        # create the n-grams\n",
    "        n_grams = ngrams(tokens, self.n)\n",
    "        n_grams = list(n_grams)\n",
    "        return n_grams\n",
    "            \n",
    "    \n",
    "    def __process_texts(self, texts: list[str], languages: list[str]):\n",
    "        for text, language in tqdm(zip(texts, languages), total=len(texts), desc=\"processing texts\"):\n",
    "            n_grams = self.__process_single(text)\n",
    "                \n",
    "            # add the n-grams to the n-grams list\n",
    "            self.n_grams[language.lower()].grams.extend(n_grams)  \n",
    "                \n",
    "                \n",
    "                \n",
    "    def __build_profile_language(self):\n",
    "        for language, n_grams in self.n_grams.items():\n",
    "            # take the count of the ngrams  \n",
    "            counts = Counter(n_grams.grams)\n",
    "            # take the top most_common_n_grams n-grams as profile\n",
    "            most_commons = counts.most_common(self.most_common_n_grams)\n",
    "            # create a profile dictionary\n",
    "            # somehow, the most_common method returns a list of tuples\n",
    "            profile = {k[0]: v for k, v in most_commons}\n",
    "            self.languages[language].profile = dict(profile)\n",
    "            \n",
    "    def preprocess_text(self, text: str):\n",
    "        n_grams = self.__process_single(text)\n",
    "        counts = Counter(n_grams)\n",
    "        most_commons = counts.most_common(self.most_common_n_grams)\n",
    "        profile = {k[0]: v for k, v in most_commons}\n",
    "        return dict(profile)\n",
    "    \n",
    "\n",
    "    \n",
    "    def fit(self, texts: list[str], languages: list[str]):\n",
    "        assert len(texts) == len(languages)\n",
    "        \n",
    "        # first create the n-grams for each text and language\n",
    "        self.__process_texts(texts, languages)\n",
    "        \n",
    "        # create the profile for the language\n",
    "        self.__build_profile_language()\n",
    "        \n",
    "        \n",
    "    def get_profile_matrix(self):\n",
    "        return self.profile_matrix\n",
    "    \n",
    "    def __distance(self, text_profile: dict, language_name: str):\n",
    "        language_profile = self.languages[language_name].profile\n",
    "        # find the common keys\n",
    "        common_keys = set(text_profile.keys()) & set(language_profile.keys())\n",
    "                \n",
    "        distance = 0.0\n",
    "        for ck in common_keys:\n",
    "            distance += abs(text_profile[ck] - language_profile[ck])\n",
    "        return distance\n",
    "    \n",
    "    def get_distance(self, text_profile: dict):\n",
    "        distances  = []\n",
    "        language_names = [lang for lang in self.languages.keys()]\n",
    "        \n",
    "        for language_name in language_names:\n",
    "            distances.append(self.__distance(text_profile, language_name))\n",
    "            \n",
    "        return distances\n",
    "        \n",
    "    \n",
    "    def __get_language_name(self, index: int):\n",
    "        return [lang for lang, idx in self.mapping.items() if idx == index][0]\n",
    "        \n",
    "    \n",
    "    def predict_single(self, text: str) -> str:\n",
    "        \"\"\"Predict the language name in lowercase\"\"\"\n",
    "        \n",
    "        # preprocess the text and get the profile\n",
    "        text_profile = self.preprocess_text(text)\n",
    "        \n",
    "        # get the distances\n",
    "        distances = self.get_distance(text_profile)\n",
    "        distances = np.array(distances)\n",
    "        \n",
    "        prediction = np.argmin(distances)\n",
    "        prediction = self.__get_language_name(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def predict(self, texts: list[str]) -> list[str]:\n",
    "        \"\"\"Predict the language of the texts\"\"\"\n",
    "        predictions = [self.predict_single(text) for text in texts]\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(clf: NGramProfileClassifier, test_dataset: list[str]) -> list[str]:\n",
    "    \"\"\"Infer the language of the text\"\"\"\n",
    "    return clf.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Since this is a classification task, I am using the usual accuracy, precision, recall and f1-score as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "def evaluate(predictions: list[str], labels: list[str]) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Evaluate the predictions\"\"\"\n",
    "    accuracy = accuracy_score(predictions, labels)\n",
    "    precision = precision_score(predictions, labels, average='macro')\n",
    "    recall = recall_score(predictions, labels, average='macro')\n",
    "    f1 = f1_score(predictions, labels, average='macro')\n",
    "    \n",
    "    # also print the classification report\n",
    "    print(classification_report(predictions, labels))\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500cf8469d764e93887f9be3e4710049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing texts:   0%|          | 0/8269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for unigrams\n",
    "unigram_clf = NGramProfileClassifier(n=1, mapping=language_to_index)\n",
    "unigram_clf.fit(train_texts, train_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00     296.0\n",
      "      Danish       0.00      0.00      0.00       0.0\n",
      "       Dutch       0.00      0.00      0.00       3.0\n",
      "     English       0.00      0.00      0.00     144.0\n",
      "      French       0.00      0.00      0.00       0.0\n",
      "      German       0.00      0.00      0.00       0.0\n",
      "       Greek       0.00      0.00      0.00       6.0\n",
      "       Hindi       0.00      0.00      0.00     801.0\n",
      "     Italian       0.00      0.00      0.00       0.0\n",
      "     Kannada       0.00      0.00      0.00     740.0\n",
      "   Malayalam       0.00      0.00      0.00      72.0\n",
      "  Portuguese       0.00      0.00      0.00       0.0\n",
      "     Russian       0.00      0.00      0.00       0.0\n",
      "     Spanish       0.00      0.00      0.00       0.0\n",
      "     Swedish       0.00      0.00      0.00       0.0\n",
      "       Tamil       0.00      0.00      0.00       6.0\n",
      "     Turkish       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00    2068.0\n",
      "   macro avg       0.00      0.00      0.00    2068.0\n",
      "weighted avg       0.00      0.00      0.00    2068.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "unigram_predictions = unigram_clf.predict(test_texts)\n",
    "unigram_scores = evaluate(unigram_predictions, test_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38a3bf85aaa4e0489571e28a7a5e309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing texts:   0%|          | 0/8269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for bigrams\n",
    "bigram_clf = NGramProfileClassifier(n=2, mapping=language_to_index)\n",
    "bigram_clf.fit(train_texts, train_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00       682\n",
      "      Danish       0.00      0.00      0.00         0\n",
      "       Dutch       0.00      0.00      0.00         0\n",
      "     English       0.04      0.01      0.02       824\n",
      "      French       0.00      0.00      0.00         0\n",
      "      German       0.00      0.00      0.00         0\n",
      "       Greek       0.00      0.00      0.00         0\n",
      "       Hindi       0.00      0.00      0.00         0\n",
      "     Italian       0.00      0.00      0.00         0\n",
      "     Kannada       0.00      0.00      0.00         0\n",
      "   Malayalam       0.07      0.01      0.02       562\n",
      "  Portuguese       0.00      0.00      0.00         0\n",
      "     Russian       0.00      0.00      0.00         0\n",
      "     Spanish       0.00      0.00      0.00         0\n",
      "     Swedish       0.00      0.00      0.00         0\n",
      "       Tamil       0.00      0.00      0.00         0\n",
      "     Turkish       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.01      2068\n",
      "   macro avg       0.01      0.00      0.00      2068\n",
      "weighted avg       0.03      0.01      0.01      2068\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "bigram_predictions = bigram_clf.predict(test_texts)\n",
    "bigram_scores = evaluate(bigram_predictions, test_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2688cd538d5d4b369b1b754fa8407b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing texts:   0%|          | 0/8269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for trigrams\n",
    "trigram_clf = NGramProfileClassifier(n=3, mapping=language_to_index)\n",
    "trigram_clf.fit(train_texts, train_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00         1\n",
      "      Danish       0.00      0.00      0.00         0\n",
      "       Dutch       0.00      0.00      0.00         1\n",
      "     English       0.07      0.02      0.03       856\n",
      "      French       0.00      0.00      0.00         0\n",
      "      German       0.00      0.00      0.00         0\n",
      "       Greek       0.00      0.00      0.00         0\n",
      "       Hindi       0.00      0.00      0.00       862\n",
      "     Italian       0.00      0.00      0.00         0\n",
      "     Kannada       0.00      0.00      0.00         0\n",
      "   Malayalam       0.03      0.01      0.02       346\n",
      "  Portuguese       0.00      0.00      0.00         1\n",
      "     Russian       0.00      0.00      0.00         0\n",
      "     Spanish       0.00      0.00      0.00         0\n",
      "     Swedish       0.00      0.00      0.00         0\n",
      "       Tamil       0.00      0.00      0.00         0\n",
      "     Turkish       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.01      2068\n",
      "   macro avg       0.01      0.00      0.00      2068\n",
      "weighted avg       0.03      0.01      0.02      2068\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shawon/Codes/oss/language-detection/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "trigram_predictions = trigram_clf.predict(test_texts)\n",
    "trigram_scores = evaluate(trigram_predictions, test_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stopwords for Malayalam\n",
      "No stopwords for Hindi\n",
      "No stopwords for Tamil\n",
      "No stopwords for Kannada\n"
     ]
    }
   ],
   "source": [
    "for language_name in language_to_index.keys():\n",
    "    try:\n",
    "        sw = stopwords.words(language_name)\n",
    "    except:\n",
    "        print(f\"No stopwords for {language_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
