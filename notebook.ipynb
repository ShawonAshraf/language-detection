{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detction from documents using n-gram profiles\n",
    "\n",
    "This notebook is an attempt at building an n-gram profile based language detector inspired by [N-gram-based text categorization Cavnar, Trenkle (1994)](https://sdmines.sdsmt.edu/upload/directory/materials/12247_20070403135416.pdf).\n",
    "\n",
    "\n",
    "\n",
    "#### BibTex entry\n",
    "```bibtex\n",
    "@inproceedings{Cavnar1994NgrambasedTC,\n",
    "  title={N-gram-based text categorization},\n",
    "  author={William B. Cavnar and John M. Trenkle},\n",
    "  year={1994},\n",
    "  url={https://api.semanticscholar.org/CorpusID:170740}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core concept\n",
    "\n",
    "According to the Zipf's Law, the most dominant words in a language are lesser in frequency than their more frequent yet less dominant counterparts. N-gram profiles are built on the idea of the ranking of the most prominent n-grams in a language.\n",
    "\n",
    "Let's assume that we have a corpus $C$ of $N$ languages. For each language $L$ in the $C$, we can then create the ranking of the most common n-grams, which will act as the n-gram profile, $R_l$ for $l$. Once the profiles for all languages have been computed, we can infer on a held out corpus, containing $S$ sentences. For each sentence $s$ in the corpus, we first create the n-gram profile of $s$, $R_s$. Then, we measure the distance in the rankings of the n-grams in $R_s$ against the n-gram profiles of all the languages. In the end, the language which will have the least distance is selected as the predicted result. For our prediction target $y_l$, \n",
    "\n",
    "$$\n",
    "y_l = min(R_{s_i} - [R_{L_1} , R_{L_2}, ... , R_{N}])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using this small corpus from Kaggle titled [Language Detection](https://www.kaggle.com/code/basilb2s/language-detection-using-nlp). It contains 17 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'examples', 'isLiveDataset', 'rai'}\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [Metadata(Language Detection)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although humans are part of nature, human acti...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1] The word nature is borrowed from the Old F...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text language\n",
       "0   Nature, in the broadest sense, is the natural...  English\n",
       "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2  The study of nature is a large, if not the onl...  English\n",
       "3  Although humans are part of nature, human acti...  English\n",
       "4  [1] The word nature is borrowed from the Old F...  English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_URL = \"https://www.kaggle.com/datasets/basilb2s/language-detection/croissant/download\"\n",
    "\n",
    "def get_croissant_dataset(dataset_url: str = DATASET_URL) -> pd.DataFrame:\n",
    "    # Fetch the Croissant JSON-LD\n",
    "    croissant_dataset = mlc.Dataset(dataset_url)\n",
    "\n",
    "    # Check what record sets are in the dataset\n",
    "    record_sets = croissant_dataset.metadata.record_sets\n",
    "\n",
    "    # Fetch the records and put them in a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        croissant_dataset.records(record_set=record_sets[0].uuid))\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns={\"Language+Detection.csv/Text\": \"text\",\n",
    "              \"Language+Detection.csv/Language\": \"language\"}, inplace=True)\n",
    "    \n",
    "    # convert the binary strings to utf-8\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "    df[\"language\"] = df[\"language\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = get_croissant_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
