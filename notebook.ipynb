{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detction from documents using n-gram profiles\n",
    "\n",
    "This notebook is an attempt at building an n-gram profile based language detector inspired by [N-gram-based text categorization Cavnar, Trenkle (1994)](https://sdmines.sdsmt.edu/upload/directory/materials/12247_20070403135416.pdf).\n",
    "\n",
    "\n",
    "\n",
    "#### BibTex entry\n",
    "```bibtex\n",
    "@inproceedings{Cavnar1994NgrambasedTC,\n",
    "  title={N-gram-based text categorization},\n",
    "  author={William B. Cavnar and John M. Trenkle},\n",
    "  year={1994},\n",
    "  url={https://api.semanticscholar.org/CorpusID:170740}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core concept\n",
    "\n",
    "According to the Zipf's Law, the most dominant words in a language are lesser in frequency than their more frequent yet less dominant counterparts. N-gram profiles are built on the idea of the ranking of the most prominent n-grams in a language.\n",
    "\n",
    "Let's assume that we have a corpus $C$ of $N$ languages. For each language $L$ in the $C$, we can then create the ranking of the most common n-grams, which will act as the n-gram profile, $R_l$ for $l$. Once the profiles for all languages have been computed, we can infer on a held out corpus, containing $S$ sentences. For each sentence $s$ in the corpus, we first create the n-gram profile of $s$, $R_s$. Then, we measure the distance in the rankings of the n-grams in $R_s$ against the n-gram profiles of all the languages. In the end, the language which will have the least distance is selected as the predicted result. For our prediction target $y_l$, \n",
    "\n",
    "$$\n",
    "y_l = min(R_{s_i} - [R_{L_1} , R_{L_2}, ... , R_{N}])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using this small corpus from Kaggle titled [Language Detection](https://www.kaggle.com/code/basilb2s/language-detection-using-nlp). It contains 17 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'examples', 'isLiveDataset', 'rai'}\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [Metadata(Language Detection)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although humans are part of nature, human acti...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1] The word nature is borrowed from the Old F...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text language\n",
       "0   Nature, in the broadest sense, is the natural...  English\n",
       "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2  The study of nature is a large, if not the onl...  English\n",
       "3  Although humans are part of nature, human acti...  English\n",
       "4  [1] The word nature is borrowed from the Old F...  English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_URL = \"https://www.kaggle.com/datasets/basilb2s/language-detection/croissant/download\"\n",
    "\n",
    "def get_croissant_dataset(dataset_url: str = DATASET_URL) -> pd.DataFrame:\n",
    "    # Fetch the Croissant JSON-LD\n",
    "    croissant_dataset = mlc.Dataset(dataset_url)\n",
    "\n",
    "    # Check what record sets are in the dataset\n",
    "    record_sets = croissant_dataset.metadata.record_sets\n",
    "\n",
    "    # Fetch the records and put them in a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        croissant_dataset.records(record_set=record_sets[0].uuid))\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns={\"Language+Detection.csv/Text\": \"text\",\n",
    "              \"Language+Detection.csv/Language\": \"language\"}, inplace=True)\n",
    "    \n",
    "    # convert the binary strings to utf-8\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "    df[\"language\"] = df[\"language\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = get_croissant_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language to index dictionary\n",
    "\n",
    "I'm assigning an integer id to each of the unique target languages in the dataset, which can then be used as index while creating language specific arrays later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': 0,\n",
       " 'Malayalam': 1,\n",
       " 'Hindi': 2,\n",
       " 'Tamil': 3,\n",
       " 'Portugeese': 4,\n",
       " 'French': 5,\n",
       " 'Dutch': 6,\n",
       " 'Spanish': 7,\n",
       " 'Greek': 8,\n",
       " 'Russian': 9,\n",
       " 'Danish': 10,\n",
       " 'Italian': 11,\n",
       " 'Turkish': 12,\n",
       " 'Sweedish': 13,\n",
       " 'Arabic': 14,\n",
       " 'German': 15,\n",
       " 'Kannada': 16}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_languages = df[\"language\"].unique()\n",
    "\n",
    "language_to_index = {language: index for index, language in enumerate(unique_languages)}\n",
    "language_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts, languages = df[\"text\"], df[\"language\"]\n",
    "train_texts, test_texts, train_languages, test_languages = train_test_split(texts, languages, test_size=0.2, random_state=42)\n",
    "\n",
    "assert len(train_texts) == len(train_languages)\n",
    "assert len(test_texts) == len(test_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Language:\n",
    "    id: int\n",
    "    name: str\n",
    "    profile: dict    \n",
    "    def __eq__(self, other: \"Language\") -> bool:\n",
    "        # two languages are equal if their profiles are the same\n",
    "        return np.array_equal(self.profile, other.profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams:\n",
    "    n: int\n",
    "    language: str\n",
    "    grams: list[str]\n",
    "    \n",
    "    def __eq__(self, other: \"NGrams\") -> bool:\n",
    "        # two n-grams are equal if their n and the language are the same\n",
    "        return self.n == other.n and self.language == other.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class NGramProfileClassifier:\n",
    "    def __init__(self, n: int, mapping: dict[str, int], most_common_n_grams: int = 25):\n",
    "        self.n = n\n",
    "        self.mapping = mapping\n",
    "        self.most_common_n_grams = most_common_n_grams\n",
    "        \n",
    "        # a dictionary of languages and their n-gram profiles\n",
    "        self.languages = {}\n",
    "        self.n_grams = {}\n",
    "        self.__populate()\n",
    "        \n",
    "        # number of languages\n",
    "        self.n_languages = len(self.mapping)\n",
    "        \n",
    "        \n",
    "    def __populate(self):\n",
    "        for language, index in self.mapping.items():\n",
    "            # lower case the language name\n",
    "            language = language.lower()\n",
    "            self.languages[language] = Language(index, language, {})\n",
    "            self.n_grams[language] = NGrams(n=self.n, language=language, grams=[])\n",
    "            \n",
    "            \n",
    "    def __process_single(self, text: str):\n",
    "        # tokenize the text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # remove stopwords\n",
    "        # tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "        # create the n-grams\n",
    "        n_grams = ngrams(tokens, self.n)\n",
    "        n_grams = list(n_grams)\n",
    "        return n_grams\n",
    "            \n",
    "    \n",
    "    def __process_texts(self, texts: list[str], languages: list[str]):\n",
    "        for text, language in tqdm(zip(texts, languages), total=len(texts), desc=\"processing texts\"):\n",
    "            n_grams = self.__process_single(text)\n",
    "                \n",
    "            # add the n-grams to the n-grams list\n",
    "            self.n_grams[language.lower()].grams.extend(n_grams)  \n",
    "                \n",
    "                \n",
    "                \n",
    "    def __build_profile_language(self):\n",
    "        for language, n_grams in self.n_grams.items():\n",
    "            # take the count of the ngrams  \n",
    "            counts = Counter(n_grams.grams)\n",
    "            # take the top most_common_n_grams n-grams as profile\n",
    "            profile = counts.most_common(self.most_common_n_grams)\n",
    "            self.languages[language].profile = profile\n",
    "            \n",
    "    def __build_profile_text(self, text: str):\n",
    "        n_grams = self.__process_single(text)\n",
    "        counts = Counter(n_grams)\n",
    "        profile = counts.most_common(self.most_common_n_grams)\n",
    "        return profile\n",
    "    \n",
    "\n",
    "    \n",
    "    def fit(self, texts: list[str], languages: list[str]):\n",
    "        assert len(texts) == len(languages)\n",
    "        \n",
    "        # first create the n-grams for each text and language\n",
    "        self.__process_texts(texts, languages)\n",
    "        \n",
    "        # create the profile for the language\n",
    "        self.__build_profile_language()\n",
    "        \n",
    "        \n",
    "    def get_profile_matrix(self):\n",
    "        return self.profile_matrix\n",
    "    \n",
    "    def __distance(self, text_profile: dict, language: str):\n",
    "        language_profile = self.languages[language].profile\n",
    "        # find the common keys\n",
    "        common_keys = set(text_profile.keys()) & set(language_profile.keys())\n",
    "                \n",
    "        distance = 0.0\n",
    "        for ck in common_keys:\n",
    "            distance += abs(text_profile[ck] - language_profile[ck])\n",
    "        return distance\n",
    "    \n",
    "    def get_distance(self, text_profile: dict):\n",
    "        language_profiles = [self.languages[language].profile for language in self.languages.keys()]\n",
    "        distances  = []\n",
    "        \n",
    "        for lp in language_profiles:\n",
    "            distances.append(self.__distance(text_profile, lp))\n",
    "            \n",
    "        return distances\n",
    "        \n",
    "    \n",
    "    def __get_language_name(self, index: int):\n",
    "        return [lang for lang, idx in self.mapping.items() if idx == index][0]\n",
    "        \n",
    "    \n",
    "    def predict_single(self, text: str) -> str:\n",
    "        \"\"\"Predict the language name in lowercase\"\"\"\n",
    "        \n",
    "        # get ngrams\n",
    "        text_n_grams = self.__process_single(text)\n",
    "        \n",
    "        # count the n-grams\n",
    "        text_n_gram_counts = Counter(text_n_grams)\n",
    "\n",
    "        # create a profile for the text\n",
    "        text_profile = text_n_gram_counts.most_common(self.most_common_n_grams)\n",
    "        \n",
    "        distances = self.get_distance(text_profile)\n",
    "        distances = np.array(distances)\n",
    "        \n",
    "        prediction = np.argmin(distances)\n",
    "        prediction = self.__get_language_name(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def predict(self, texts: list[str]) -> list[str]:\n",
    "        \"\"\"Predict the language of the texts\"\"\"\n",
    "        predictions = [self.predict_single(text) for text in texts]\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(clf: NGramProfileClassifier, test_dataset: list[str]) -> list[str]:\n",
    "    \"\"\"Infer the language of the text\"\"\"\n",
    "    return clf.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Since this is a classification task, I am using the usual accuracy, precision, recall and f1-score as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "def evaluate(predictions: list[str], labels: list[str]) -> tuple[float, float, float, float]:\n",
    "    \"\"\"Evaluate the predictions\"\"\"\n",
    "    accuracy = accuracy_score(predictions, labels)\n",
    "    precision = precision_score(predictions, labels, average='macro')\n",
    "    recall = recall_score(predictions, labels, average='macro')\n",
    "    f1 = f1_score(predictions, labels, average='macro')\n",
    "    \n",
    "    # also print the classification report\n",
    "    print(classification_report(predictions, labels))\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dfbc7aad5142e985b6a41e09dccd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing texts:   0%|          | 0/8269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for unigrams\n",
    "unigram_clf = NGramProfileClassifier(n=1, mapping=language_to_index)\n",
    "unigram_clf.fit(train_texts, train_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m unigram_predictions = \u001b[43munigram_clf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m unigram_scores = evaluate(unigram_predictions, test_languages)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mNGramProfileClassifier.predict\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Predict the language of the texts\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     predictions = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mNGramProfileClassifier.predict_single\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# create a profile for the text\u001b[39;00m\n\u001b[32m    111\u001b[39m text_profile = text_n_gram_counts.most_common(\u001b[38;5;28mself\u001b[39m.most_common_n_grams)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m distances = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_profile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m distances = np.array(distances)\n\u001b[32m    116\u001b[39m prediction = np.argmin(distances)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mNGramProfileClassifier.get_distance\u001b[39m\u001b[34m(self, text_profile)\u001b[39m\n\u001b[32m     89\u001b[39m distances  = []\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lp \u001b[38;5;129;01min\u001b[39;00m language_profiles:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     distances.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m distances\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mNGramProfileClassifier.__distance\u001b[39m\u001b[34m(self, text_profile, language)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__distance\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_profile: \u001b[38;5;28mdict\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     language_profile = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m.profile\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# find the common keys\u001b[39;00m\n\u001b[32m     80\u001b[39m     common_keys = \u001b[38;5;28mset\u001b[39m(text_profile.keys()) & \u001b[38;5;28mset\u001b[39m(language_profile.keys())\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "unigram_predictions = unigram_clf.predict(test_texts)\n",
    "unigram_scores = evaluate(unigram_predictions, test_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bigrams\n",
    "bigram_clf = NGramProfileClassifier(n=2, mapping=language_to_index)\n",
    "bigram_clf.fit(train_texts, train_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trigrams\n",
    "trigram_clf = NGramProfileClassifier(n=3, mapping=language_to_index)\n",
    "trigram_clf.fit(train_texts, train_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "a = {\"a\": 45, \"b\": 10, \"c\": 100, \"d\": 1000}\n",
    "b = {\"a\": 35, \"b\": 2}\n",
    "\n",
    "# find common keys\n",
    "common_keys = set(a.keys()) & set(b.keys())\n",
    "common_keys = list(common_keys)\n",
    "common_keys\n",
    "\n",
    "distance = 0.0\n",
    "for ck in common_keys:\n",
    "    distance += abs(a[ck] - b[ck])\n",
    "    \n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
